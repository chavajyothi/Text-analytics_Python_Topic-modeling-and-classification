{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The python note book has two section.\n",
    "\n",
    "1. topic modeling\n",
    "2. multiclassification model\n",
    "\n",
    "Topic modeling is adopted to lable the text data. The labled data is used to develop a multiclassification model.\n",
    "we adopted several classification algoritham and checked there metrics.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data.\n",
    "The data is downloaded from https://www.kaggle.com/benhamner/nips-papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = pd.read_csv(\"C:/Jyothi/Software training/Python/TOPIC modeling/papers.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'year', 'title', 'event_type', 'pdf_name', 'abstract',\n",
       "       'paper_text'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7241, 7)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers=papers.drop([\"id\",\"event_type\",'pdf_name'],axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>paper_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1987</td>\n",
       "      <td>Self-Organization of Associative Database and ...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1987</td>\n",
       "      <td>A Mean Field Theory of Layer IV of Visual Cort...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1988</td>\n",
       "      <td>Storing Covariance by the Associative Long-Ter...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1994</td>\n",
       "      <td>Bayesian Query Construction for Neural Network...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>Bayesian Query Construction for Neural\\nNetwor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1994</td>\n",
       "      <td>Neural Network Ensembles, Cross Validation, an...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>Neural Network Ensembles, Cross\\nValidation, a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year                                              title          abstract  \\\n",
       "0  1987  Self-Organization of Associative Database and ...  Abstract Missing   \n",
       "1  1987  A Mean Field Theory of Layer IV of Visual Cort...  Abstract Missing   \n",
       "2  1988  Storing Covariance by the Associative Long-Ter...  Abstract Missing   \n",
       "3  1994  Bayesian Query Construction for Neural Network...  Abstract Missing   \n",
       "4  1994  Neural Network Ensembles, Cross Validation, an...  Abstract Missing   \n",
       "\n",
       "                                          paper_text  \n",
       "0  767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...  \n",
       "1  683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...  \n",
       "2  394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...  \n",
       "3  Bayesian Query Construction for Neural\\nNetwor...  \n",
       "4  Neural Network Ensembles, Cross\\nValidation, a...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...\n",
       "1     683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...\n",
       "2     394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...\n",
       "3     Bayesian Query Construction for Neural\\nNetwor...\n",
       "4     Neural Network Ensembles, Cross\\nValidation, a...\n",
       "5     U sing a neural net to instantiate a\\ndeformab...\n",
       "6     Plasticity-Mediated Competitive Learning\\n\\nTe...\n",
       "7     ICEG Morphology Classification using an\\nAnalo...\n",
       "8     Real-Time Control of a Tokamak Plasma\\nUsing N...\n",
       "9     Real-Time Control of a Tokamak Plasma\\nUsing N...\n",
       "10    Learning To Play the Game of Chess\\n\\nSebastia...\n",
       "11    Multidimensional Scaling and Data Clustering\\n...\n",
       "12    An experimental comparison\\nof recurrent neura...\n",
       "13    133\\n\\nTRAINING MULTILAYER PERCEPTRONS WITH TH...\n",
       "14    Interference in Learning Internal\\nModels of I...\n",
       "15    Active Learning with Statistical Models\\n\\nDav...\n",
       "16    A Rapid Graph-based Method for\\nArbitrary Tran...\n",
       "17    Ocular Dominance and Patterned Lateral\\nConnec...\n",
       "18    Associative Decorrelation Dynamics:\\nA Theory ...\n",
       "19    A Connectionist Technique for Accelerated\\nTex...\n",
       "Name: paper_text, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers.paper_text.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count          7241\n",
       "unique         7237\n",
       "top       \n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "freq              2\n",
       "Name: paper_text, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers.paper_text.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation\n",
    "papers['paper_text_processed'] = papers['paper_text'].map(lambda x: re.sub('[,\\.!?]', '', x))\n",
    "papers['paper_text_processed'] = papers['paper_text_processed'].map(lambda x: re.sub('[\\n]', ' ', x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers['paper_text_processed'] = papers['paper_text_processed'].map(lambda x: x.lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     767  self-organization of associative database...\n",
       "1     683  a mean field theory of layer iv of visual...\n",
       "2     394  storing covariance by the associative lon...\n",
       "3     bayesian query construction for neural network...\n",
       "4     neural network ensembles cross validation and ...\n",
       "5     u sing a neural net to instantiate a deformabl...\n",
       "6     plasticity-mediated competitive learning  terr...\n",
       "7     iceg morphology classification using an analog...\n",
       "8     real-time control of a tokamak plasma using ne...\n",
       "9     real-time control of a tokamak plasma using ne...\n",
       "10    learning to play the game of chess  sebastian ...\n",
       "11    multidimensional scaling and data clustering  ...\n",
       "12    an experimental comparison of recurrent neural...\n",
       "13    133  training multilayer perceptrons with the ...\n",
       "14    interference in learning internal models of in...\n",
       "15    active learning with statistical models  david...\n",
       "16    a rapid graph-based method for arbitrary trans...\n",
       "17    ocular dominance and patterned lateral connect...\n",
       "18    associative decorrelation dynamics: a theory o...\n",
       "19    a connectionist technique for accelerated text...\n",
       "Name: paper_text_processed, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers['paper_text_processed'].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'767  self-organization of associative database and its applications hisashi suzuki and suguru arimoto osaka university toyonaka osaka 560 japan abstract an efficient method of self-organizing associative databases is proposed together with applications to robot eyesight systems the proposed databases can associate any input with some output in the first half part of discussion an algorithm of self-organization is proposed from an aspect of hardware it produces a new style of neural network in the latter half part an applicability to handwritten letter recognition and that to an autonomous mobile robot system are demonstrated  introduction let a mapping f : x -+ y be given here x is a finite or infinite set and y is another finite or infinite set a learning machine observes any set of pairs (x y) sampled randomly from x x y (x x y means the cartesian product of x and y) and it computes some estimate j : x -+ y of f to make small the estimation error in some measure usually we say that: the faster the decrease of estimation error with increase of the number of samples the better the learning machine however such expression on performance is incomplete since it lacks consideration on the candidates of j of j assumed preliminarily then how should we find out good learning machines to clarify this conception let us discuss for a while on some types of learning machines and let us advance the understanding of the self-organization of associative database   parameter type an ordinary type of learning machine assumes an equation relating x\\'s and y\\'s with parameters being indefinite namely a structure of f it is equivalent to define implicitly a set f of candidates of (f is some subset of mappings from x to y) and it computes values of the parameters based on the observed samples we call such type a parameter type for a learning machine defined well if f 3 f j approaches f as the number of samples increases in the alternative case however some estimation error remains eternally thus a problem of designing a learning machine returns to find out a proper structure of f in this sense on the other hand the assumed structure of f is demanded to be as compact as possible to achieve a fast learning in other words the number of parameters should be small since if the parameters are few some j can be uniquely determined even though the observed samples are few however this demand of being proper contradicts to that of being compact consequently in the parameter type the better the compactness of the assumed structure that is proper the better the learning machine this is the most elementary conception when we design learning machines   1   universality and ordinary neural networks now suppose that a sufficient knowledge on f is given though j itself is unknown in this case it is comparatively easy to find out proper and compact structures of j in the alternative case however it is sometimes difficult a possible solution is to give up the compactness and assume an almighty structure that can cover various 1\\'s a combination of some orthogonal bases of the infinite dimension is such a structure neural networks 1 2 are its approximations obtained by truncating finitely the dimension for implementation   american institute of physics 1988  \\x0c768 a main topic in designing neural networks is to establish such desirable structures of 1 this work includes developing practical procedures that compute values of coefficients from the observed samples such discussions are :flourishing since 1980 while many efficient methods have been proposed recently even hardware units computing coefficients in parallel for speed-up are sold eg anza mark iii odyssey and e-1 nevertheless in neural networks there always exists a danger of some error remaining eternally in estimating / precisely speaking suppose that a combination of the bases of a finite number can define a structure of 1 essentially in other words suppose that f 3 / or 1 is located near f in such case the estimation error is none or negligible however if 1 is distant from f the estimation error never becomes negligible indeed many researches report that the following situation appears when 1 is too complex once the estimation error converges to some value (> 0) as the number of samples increases it decreases hardly even though the dimension is heighten this property sometimes is a considerable defect of neural networks   recursi ve type the recursive type is founded on another methodology of learning that should be as follows at the initial stage of no sample the set fa (instead of notation f) of candidates of i equals to the set of all mappings from x to y after observing the first sample (xl yl) e x x y fa is reduced to fi so that i(xt) = yl for any i e f after observing the second sample (x2\\' y2) e x x y fl is further reduced to f2 so that i(xt) = yl and i(x2) = y2 for any i e f thus the candidate set f becomes gradually small as observation of samples proceeds the after observing i-samples which we write is one of the most likelihood estimation of 1 selected in fi; hence contrarily to the parameter type the recursive type guarantees surely that j approaches to 1 as the number of samples increases the recursive type if observes a sample (x\" yd rewrites values 1-l(x)s to i(x)\\'s for some x\\'s correlated to the sample hence this type has an architecture composed of a rule for rewriting and a free memory space such architecture forms naturally a kind of database that builds up management systems of data in a self-organizing way however this database differs from ordinary ones in the following sense it does not only record the samples already observed but computes some estimation of l(x) for any x e x we call such database an associative database the first subject in constructing associative databases is how we establish the rule for rewri ting for this purpose we adap t a measure called the dissimilari ty here a dissimilari ty means a mapping d : x x x -+ {reals > o} such that for any (x x) e x x x d(x x) > 0 whenever l(x) t /(x) however it is not necessarily defined with a single formula it is definable with for example a collection of rules written in forms of \"if  then  \" the dissimilarity d defines a structure of 1 locally in x x y hence even though the knowledge on f is imperfect we can re:flect it on d in some heuristic way hence contrarily to neural networks it is possible to accelerate the speed of learning by establishing d well especially we can easily find out simple d\\'s for those l\\'s which process analogically information like a human (see the applications in this paper) and for such /\\'s the recursive type shows strongly its effectiveness we denote a sequence of observed samples by (xl yd (x2\\' y2) one of the simplest constructions of associative databases after observing i-samples (i = 12) is as follows  i  i\"  i  algorithm 1 at the initial stage let so be the empty set for every i = 12\"   let i-l(x) for any x e x equal some y* such that (x*y*) e s-l and  d(x x*) =  min (%y)es-t  d(x x)   furthermore add (x\" y) to s;-l to produce sa ie s = s_l u {(x\"  (1)  yn  \\x0c769  another version improved to economize the memory is as follows  algorithm 2 at the initial stage let so be composed of an arbitrary element in x x y for every i = 12\"\" let ii-lex) for any x e x equal some y such that (x y) e si-l and d(x x) =  min  d(x x)   (ii)es-l  furthermore if ii-l(xi) # yi then let si = si-l or add (xi yi) to si-l to produce si ie si = si-l u {(xi yi)}\\' in either construction ii approaches to f as i increases however the computation time grows proportionally to the size of si the second subject in constructing associative databases is what addressing rule we should employ to economize the computation time in the subsequent chapters a construction of associative database for this purpose is proposed it manages data in a form of binary tree  self-organization of associative database given a sample sequence (xl yl) (x2\\' y2)  \" the algorithm for constructing associative database is as follows  algorithm 3\\'  step i(initialization): let (x[root] y[root]) = (xl yd here x[] and y[] are variables assigned for respective nodes to memorize data furthermore let t = 1 step 2: increase t by 1 and put x in after reset a pointer n to the root repeat the following until n arrives at some terminal node ie leaf notations nand d(xt x[n)) let n  n mean the descendant nodes of n =n otherwise let n =n  if d(x\" r[n)) ~  step 3: display yin] as the related information next put y in if yin] = y\" back to step 2 otherwise first establish new descendant nodes n and n secondly let  (x[n] yin)) (x[n] yin))  (x[n] yin)) (xt y)  (2) (3)  finally back to step 2 here the loop of step 2-3 can be stopped at any time and also can be continued now suppose that gate elements namely artificial \"synapses\" that play the role of branching by d are prepared then we obtain a new style of neural network with gate elements being randomly connected by this algorithm  letter recognition recen tly the vertical slitting method for recognizing typographic english letters3  the elastic matching method for recognizing hand written discrete english letters4  the global training and fuzzy logic search method for recognizing chinese characters written in square styles etc are published the self-organization of associative database realizes the recognition of handwritten continuous english letters  \\x0c770  9 /wn\"  nov  ~ ~ ~ -xk :lat  ~~ ~ ~~~  dw1lo\\'  ~~~~~of~~  ~~~ 4-~~4fig 1 source document 2~~---------------\\'  loo~---------------\\'  h  o  o fig 2 windowing  1000  2000  3000  4000  number of samples  o  1000  2000  3000  4000  nualber of sampl es  fig 3 an experiment result  an image scanner takes a document image (fig 1) the letter recognizer uses a parallelogram window that at least can cover the maximal letter (fig 2) and processes the sequence of letters while shifting the window that is the recognizer scans a word in a slant direction and it places the window so that its left vicinity may be on the first black point detected then the window catches a letter and some part of the succeeding letter if recognition of the head letter is performed its end position namely the boundary line between two letters becomes known hence by starting the scanning from this boundary and repeating the above operations the recognizer accomplishes recursively the task thus the major problem comes to identifying the head letter in the window considering it we define the following  regard window images as x\\'s and define x accordingly  for a (x x) e x x x denote by b a black point in the left area from the boundary on window image x project each b onto window image x then measure the euclidean distance 6 between fj and a black point b on x being the closest to b let d(x x) be the summation of 6\\'s for all black points b\\'s on x divided by the number of b\\'s  regard couples of the \"reading\" and the position of boundary as y\\'s and define y accordingly an operator teaches the recognizer in interaction the relation between window image and reading& boundary with algorithm 3 precisely if the recalled reading is incorrect the operator teaches a correct reading via the console moreover if the boundary position is incorrect he teaches a correct position via the mouse fig 1 shows partially a document image used in this experiment fig 3 shows the change of the number of nodes and that of the recognition rate defined as the relative frequency of correct answers in the past 1000 trials speciiications of the window are height = 20dot width = 10dot and slant angular = 68deg in this example the levels of tree were distributed in 6-19 at time 4000 and the recognition rate converged to about 74% experimentally the recognition rate converges to about 60-85% in most cases and to 95% at a rare case however it does not attain 100% since eg \"c\" and \"e\" are not distinguishable because of excessive lluctuation in writing if the consistency of the x y-relation is not assured like this the number of nodes increases endlessly (d fig 3) hence it is clever to stop the learning when the recognition rate attains some upper limit to improve further the recognition rate we must consider the spelling of words it is one of future subjects  \\x0c771  obstacle avoiding movement various systems of camera type autonomous mobile robot are reported flourishingly6-1o the system made up by the authors (fig 4) also belongs to this category now in mathematical methodologies we solve usually the problem of obstacle avoiding movement as a cost minimization problem under some cost criterion established artificially contrarily the self-organization of associative database reproduces faithfully the cost criterion of an operator therefore motion of the robot after learning becomes very natural now the length width and height of the robot are all about o7m and the weight is about 30kg the visual angle of camera is about 55deg the robot has the following three factors of motion it turns less than 30deg advances less than 1m and controls speed less than 3km/h the experiment was done on the passageway of wid th 25m inside a building which the authors\\' laboratories exist in (fig 5) because of an experimental intention we arrange boxes smoking stands gas cylinders stools handcarts etc on the passage way at random we let the robot take an image through the camera recall a similar image and trace the route preliminarily recorded on it for this purpose we define the following  let the camera face 28deg downward to take an image and process it through a low pass filter scanning vertically the filtered image from the bottom to the top search the first point c where the luminance changes excessively then su bstitu te all points from the bottom to c for white and all points from c to the top for black (fig 6) (if no obstacle exists just in front of the robot the white area shows the \\'\\'free\\'\\' area where the robot can move around) regard binary 32 x 32dot images processed thus as x\\'s and define x accordingly  for every (x x) e x x x let d(x x) be the number of black points on the exclusive-or image between x and x  regard as y\\'s the images obtained by drawing routes on images x\\'s and define y accordingly the robot superimposes on the current camera image x the route recalled for x and inquires the operator instructions the operator judges subjectively whether the suggested route is appropriate or not in the negative answer he draws a desirable route on x with the mouse to teach a new y to the robot this operation defines implicitly a sample sequence of (x y) reflecting the cost criterion of the operator  ::l\"  -  iibube  _ -  22  11  roan  12  {-  13  stationary uni t  fig 4 configuration of autonomous mobile robot system  ~  i    23  24  north 14  rmbi ie unit (robot)  -  roan  y  t  fig 5 experimental environment  \\x0c772  wall  camera image  preprocessing  a  ::: fa    preprocessing  0  o  course suggest ion      search  a  fig 6 processing for obstacle avoiding movement  x  fig 1 processing for position identification we define the satisfaction rate by the relative frequency of acceptable suggestions of route in the past 100 trials in a typical experiment the change of satisfaction rate showed a similar tendency to fig 3 and it attains about 95% around time 800 here notice that the rest 5% does not mean directly the percentage of collision (in practice we prevent the collision by adopting some supplementary measure) at time 800 the number of nodes was 145 and the levels of tree were distributed in 6-17 the proposed method reflects delicately various characters of operator for example a robot trained by an operator 0 moves slowly with enough space against obstacles while one trained by another operator 0\\' brushes quickly against obstacles this fact gives us a hint on a method of printing \"characters\" into machines position identification the robot can identify its position by recalling a similar landscape with the position data to a camera image for this purpose in principle it suffices to regard camera images and position data as x\\'s and y\\'s respectively however the memory capacity is finite in actual compu ters hence we cannot but compress the camera images at a slight loss of information such compression is admittable as long as the precision of position identification is in an acceptable area thus the major problem comes to find out some suitable compression method in the experimental environment (fig 5) juts are on the passageway at intervals of 36m and each section between adjacent juts has at most one door the robot identifies roughly from a surrounding landscape which section itself places in and it uses temporarily a triangular surveying technique if an exact measure is necessary to realize the former task we define the following   turn the camera to take a panorama image of 360deg scanning horizontally the center line substitute the points where the luminance excessively changes for black and the other points for white (fig 1) regard binary 360dot line images processed thus as x\\'s and define x accordingly  for every (x x) e x x x project each black point a on x onto x and measure the euclidean distance 6 between a and a black point a on x being the closest to a let the summation of 6 be s similarly calculate s by exchanging the roles of x and x denoting the numbers of a\\'s and a\\'s respectively by nand n define  \\x0c773  d(x x) =  ~(~ + ~) 2 n n  (4)   regard positive integers labeled on sections as y\\'s (cf fig 5) and define y accordingly in the learning mode the robot checks exactly its position with a counter that is reset periodically by the operator the robot runs arbitrarily on the passageways within 18m area and learns the relation between landscapes and position data (position identification beyond 18m area is achieved by crossing plural databases one another) this task is automatic excepting the periodic reset of counter namely it is a kind of learning without teacher we define the identification rate by the relative frequency of correct recalls of position data in the past 100 trials in a typical example it converged to about 83% around time 400 at time 400 the number of levels was 202 and the levels oftree were distributed in 522 since the identification failures of 17% can be rejected by considering the trajectory no pro blem arises in practical use in order to improve the identification rate the compression ratio of camera images must be loosened such possibility depends on improvement of the hardware in the future fig 8 shows an example of actual motion of the robot based on the database for obstacle avoiding movement and that for position identification this example corresponds to a case of moving from 14 to 23 in fig 5 here the time interval per frame is about 40sec  ~ ~ ( ;~\"i ~  \"  \"    i  i     \"  i\\'  \\'1 t  ;  i  -:     \\'ii  fig 8 actual motion of the robot  \\x0c774  conclusion a method of self-organizing associative databases was proposed with the application to robot eyesight systems the machine decomposes a global structure unknown into a set of local structures known and learns universally any input-output response this framework of problem implies a wide application area other than the examples shown in this paper a defect of the algorithm 3 of self-organization is that the tree is balanced well only for a subclass of structures of f a subject imposed us is to widen the class a probable solution is to abolish the addressing rule depending directly on values of d and instead to establish another rule depending on the distribution function of values of d it is now under investigation  references 1 hopfield j j and d w tank \"computing with neural circuit: a model/\\'  science 233 (1986) pp 625-633 2 rumelhart d e et al \"learning representations by back-propagating errors\" nature 323 (1986) pp 533-536  3 hull j j \"hypothesis generation in a computational model for visual word recognition\" ieee expert fall (1986) pp 63-70 4 kurtzberg j m \"feature analysis for symbol recognition by elastic matching\" ibm j res develop 31-1 (1987) pp 91-95  5 wang q r and c y suen \"large tree classifier with heuristic search and global training\" ieee trans pattern anal & mach intell pami 9-1 (1987) pp 91-102 6 brooks r a et al \"self calibration of motion and stereo vision for mobile robots\" 4th int symp of robotics research (1987) pp 267-276 7 goto y and a stentz \"the cmu system for mobile robot navigation\" 1987 ieee int conf on robotics & automation (1987) pp 99-105 8 madarasz r et al \"the design of an autonomous vehicle for the disabled\" ieee jour of robotics & automation ra 2-3 (1986) pp 117-125 9 triendl e and d j kriegman \"stereo vision and navigation within buildings\" 1987 ieee int conf on robotics & automation (1987) pp 1725-1730 10 turk m a et al \"video road-following for the autonomous land vehicle\" 1987 ieee int conf on robotics & automation (1987) pp 273-279  \\x0c'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers['paper_text_processed'].values.take(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_string = ','.join(list(papers['paper_text_processed'].values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(background_color=\"white\", max_words=5000, contour_width=3, contour_color='steelblue')\n",
    "# Generate a word cloud\n",
    "wordcloud.generate(long_string)\n",
    "# Visualize the word cloud\n",
    "wordcloud.to_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "converting the text data to the digital foem using bag of words. \n",
    "\n",
    "Topic Modeling is kind of unsuperbidsed machine learning algoritham to lable the text data.First step in this process is chossing the number of clusters/topics. If we have a domain knowledge we can do that. For example, in the example below its the articles published in a journal. The journals often will have the list of prior defined themes. Based on those theme we can define the number of topics.\n",
    "They are two assumption under the topic modeling algorithams:\n",
    "1. each topic is the mixture of all words each carrying a specific weightage (the shape of the output after the execution of the alogoritham is number of topics*total feature names) in this example this value is 5*113277\n",
    "2. each document is the mixture of all topics each carrying a specific weight.We choose the topic with high weighage as lable of the document. (the shape of the output after the execution of the alogoritham is total number of documents*number of topics) in this example this value is 7241*5\n",
    "\n",
    "There are various algorithams to conduct topic modeling include Latent Semantic Analysis (LSA/LSI), Probabilistic Latent Semantic Analysis (pLSA), and Latent Dirichlet Allocation (LDA). In this example we are adopting LDA.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vectorizer = CountVectorizer(max_df=0.8, min_df=2, stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_data_matrix = count_vectorizer.fit_transform((papers['paper_text_processed']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<7241x113227 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 6221117 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_data_matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation as LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_topics = 5\n",
    "\n",
    "\n",
    "lda = LDA(n_components=number_topics, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_fit =lda.fit(count_data_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113227"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(count_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "av2\n",
      "laurence\n",
      "amortizes\n",
      "ljk\n",
      "682687\n",
      "darkand\n",
      "film\n",
      "dj1\n",
      "536539\n",
      "cll\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "for i in range(10):\n",
    "    random_id = random.randint(0,len(count_vectorizer.get_feature_names()))\n",
    "    print(count_vectorizer.get_feature_names()[random_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.36286309e+02, 4.16322891e+02, 2.51404068e+01, ...,\n",
       "       2.33407009e+01, 8.17368166e+00, 2.03271719e-01])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_topic = lda.components_[0]\n",
    "first_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.36286309e+02, 4.16322891e+02, 2.51404068e+01, ...,\n",
       "        2.33407009e+01, 8.17368166e+00, 2.03271719e-01],\n",
       "       [2.21774024e+03, 5.69279575e+02, 8.73124352e+01, ...,\n",
       "        2.23882719e+01, 1.19273526e+00, 4.19434131e+00],\n",
       "       [1.70080356e+03, 1.32484571e+03, 1.58529871e+02, ...,\n",
       "        2.01043832e-01, 2.00380432e-01, 2.00213449e-01],\n",
       "       [7.43531025e+02, 3.74782997e+02, 8.48988947e+01, ...,\n",
       "        2.16979164e-01, 2.21768893e-01, 2.01633277e-01],\n",
       "       [4.05638867e+02, 4.39768824e+02, 1.61183921e+01, ...,\n",
       "        8.53004110e-01, 2.11433752e-01, 2.00540242e-01]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(113227,)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_topic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 69675,  80842,  40990,  69112,  35299,  80865, 101674,  29257,\n",
       "        70342,  72646], dtype=int64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_topic_words = first_topic.argsort()[-10:]\n",
    "top_topic_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear\n",
      "optimal\n",
      "convex\n",
      "let\n",
      "bound\n",
      "optimization\n",
      "theorem\n",
      "algorithms\n",
      "log\n",
      "matrix\n"
     ]
    }
   ],
   "source": [
    "for i in top_topic_words:\n",
    "    print(count_vectorizer.get_feature_names()[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7241, 5)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_values = lda.transform(count_data_matrix)\n",
    "topic_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers['Topic'] = topic_values.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>paper_text</th>\n",
       "      <th>paper_text_processed</th>\n",
       "      <th>Topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1987</td>\n",
       "      <td>Self-Organization of Associative Database and ...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...</td>\n",
       "      <td>767  self-organization of associative database...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1987</td>\n",
       "      <td>A Mean Field Theory of Layer IV of Visual Cort...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...</td>\n",
       "      <td>683  a mean field theory of layer iv of visual...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1988</td>\n",
       "      <td>Storing Covariance by the Associative Long-Ter...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...</td>\n",
       "      <td>394  storing covariance by the associative lon...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1994</td>\n",
       "      <td>Bayesian Query Construction for Neural Network...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>Bayesian Query Construction for Neural\\nNetwor...</td>\n",
       "      <td>bayesian query construction for neural network...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1994</td>\n",
       "      <td>Neural Network Ensembles, Cross Validation, an...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>Neural Network Ensembles, Cross\\nValidation, a...</td>\n",
       "      <td>neural network ensembles cross validation and ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year                                              title          abstract  \\\n",
       "0  1987  Self-Organization of Associative Database and ...  Abstract Missing   \n",
       "1  1987  A Mean Field Theory of Layer IV of Visual Cort...  Abstract Missing   \n",
       "2  1988  Storing Covariance by the Associative Long-Ter...  Abstract Missing   \n",
       "3  1994  Bayesian Query Construction for Neural Network...  Abstract Missing   \n",
       "4  1994  Neural Network Ensembles, Cross Validation, an...  Abstract Missing   \n",
       "\n",
       "                                          paper_text  \\\n",
       "0  767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...   \n",
       "1  683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...   \n",
       "2  394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...   \n",
       "3  Bayesian Query Construction for Neural\\nNetwor...   \n",
       "4  Neural Network Ensembles, Cross\\nValidation, a...   \n",
       "\n",
       "                                paper_text_processed  Topic  \n",
       "0  767  self-organization of associative database...      4  \n",
       "1  683  a mean field theory of layer iv of visual...      3  \n",
       "2  394  storing covariance by the associative lon...      3  \n",
       "3  bayesian query construction for neural network...      1  \n",
       "4  neural network ensembles cross validation and ...      1  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFIDF = TfidfVectorizer()\n",
    "text_TF = TFIDF.fit_transform(papers[\"paper_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method spmatrix.get_shape of <7241x233506 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 7453306 stored elements in Compressed Sparse Row format>>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_TF.get_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    text_TF, papers['Topic'], test_size=0.3, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method spmatrix.get_shape of <5068x233506 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 5230482 stored elements in Compressed Sparse Row format>>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.get_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Developing models using the lableed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = ['LogisticRegression','RandomForestClassifier','AdaBoostClassifier','K Nearest Neighbor','XGBoost']\n",
    "ac_score_list =[]\n",
    "p_score_list=[]\n",
    "r_score_list=[]\n",
    "f1_score_list =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kesav\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5.91 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "m1=OneVsRestClassifier(LogisticRegression()).fit(X_train, y_train)\n",
    "y_pred = m1.predict(X_test)\n",
    "ac_score_list.append(accuracy_score(y_test, y_pred))\n",
    "p_score_list.append(precision_score(y_test, y_pred, average='macro'))\n",
    "r_score_list.append(recall_score(y_test, y_pred, average='macro'))\n",
    "f1_score_list.append(f1_score(y_test, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.884068589620238,\n",
       " 0.6118549634312835,\n",
       " 0.7143641527144888,\n",
       " 0.6680189191568877,\n",
       " 0.8409698042766827]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kesav\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.42 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "m2=RandomForestClassifier(random_state=3).fit(X_train, y_train)\n",
    "y_pred = m2.predict(X_test)\n",
    "ac_score_list.append(accuracy_score(y_test, y_pred))\n",
    "p_score_list.append(precision_score(y_test, y_pred, average='macro'))\n",
    "r_score_list.append(recall_score(y_test, y_pred, average='macro'))\n",
    "f1_score_list.append(f1_score(y_test, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "m3=AdaBoostClassifier(random_state=3).fit(X_train, y_train)\n",
    "y_pred = m3.predict(X_test)\n",
    "ac_score_list.append(accuracy_score(y_test, y_pred))\n",
    "p_score_list.append(precision_score(y_test, y_pred, average='macro'))\n",
    "r_score_list.append(recall_score(y_test, y_pred, average='macro'))\n",
    "f1_score_list.append(f1_score(y_test, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8.53 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "m4= KNeighborsClassifier().fit(X_train, y_train)\n",
    "y_pred = m4.predict(X_test)\n",
    "ac_score_list.append(accuracy_score(y_test, y_pred))\n",
    "p_score_list.append(precision_score(y_test, y_pred, average='macro'))\n",
    "r_score_list.append(recall_score(y_test, y_pred, average='macro'))\n",
    "f1_score_list.append(f1_score(y_test, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 10min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "m5=XGBClassifier().fit(X_train, y_train)\n",
    "y_pred = m5.predict(X_test)\n",
    "ac_score_list.append(accuracy_score(y_test, y_pred))\n",
    "p_score_list.append(precision_score(y_test, y_pred, average='macro'))\n",
    "r_score_list.append(recall_score(y_test, y_pred, average='macro'))\n",
    "f1_score_list.append(f1_score(y_test, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_comparison_df = pd.DataFrame([model_name, ac_score_list, p_score_list, r_score_list, f1_score_list]).T\n",
    "model_comparison_df.columns = ['model_name', 'accuracy_score', 'precision_score', 'recall_score', 'f1_score']\n",
    "model_comparison_df = model_comparison_df.sort_values(by='f1_score', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>accuracy_score</th>\n",
       "      <th>precision_score</th>\n",
       "      <th>recall_score</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.886332</td>\n",
       "      <td>0.888675</td>\n",
       "      <td>0.880722</td>\n",
       "      <td>0.884069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.840313</td>\n",
       "      <td>0.844064</td>\n",
       "      <td>0.838506</td>\n",
       "      <td>0.84097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Decsision Tree</td>\n",
       "      <td>0.710078</td>\n",
       "      <td>0.714677</td>\n",
       "      <td>0.714779</td>\n",
       "      <td>0.714364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.673263</td>\n",
       "      <td>0.737904</td>\n",
       "      <td>0.650032</td>\n",
       "      <td>0.668019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SGDClassifier</td>\n",
       "      <td>0.617579</td>\n",
       "      <td>0.649605</td>\n",
       "      <td>0.602568</td>\n",
       "      <td>0.611855</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           model_name accuracy_score precision_score recall_score  f1_score\n",
       "0  LogisticRegression       0.886332        0.888675     0.880722  0.884069\n",
       "4            AdaBoost       0.840313        0.844064     0.838506   0.84097\n",
       "2      Decsision Tree       0.710078        0.714677     0.714779  0.714364\n",
       "3       Random Forest       0.673263        0.737904     0.650032  0.668019\n",
       "1       SGDClassifier       0.617579        0.649605     0.602568  0.611855"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_comparison_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "absl-py==0.9.0\n",
      "alabaster==0.7.12\n",
      "anaconda-client==1.7.2\n",
      "anaconda-navigator==1.9.12\n",
      "anaconda-project==0.8.2\n",
      "asn1crypto==0.24.0\n",
      "astor==0.8.1\n",
      "astroid==2.1.0\n",
      "astropy==3.1\n",
      "astunparse==1.6.3\n",
      "atomicwrites==1.2.1\n",
      "attrs==18.2.0\n",
      "Babel==2.6.0\n",
      "backcall==0.1.0\n",
      "backports.os==0.1.1\n",
      "backports.shutil-get-terminal-size==1.0.0\n",
      "beautifulsoup4==4.6.3\n",
      "bitarray==0.8.3\n",
      "bkcharts==0.2\n",
      "blaze==0.11.3\n",
      "bleach==3.0.2\n",
      "bokeh==1.0.2\n",
      "boto==2.49.0\n",
      "Bottleneck==1.2.1\n",
      "certifi==2020.6.20\n",
      "cffi==1.11.5\n",
      "chardet==3.0.4\n",
      "Click==7.0\n",
      "cloudpickle==0.6.1\n",
      "clyent==1.2.2\n",
      "colorama==0.4.1\n",
      "comtypes==1.1.7\n",
      "conda==4.8.4\n",
      "conda-build==3.17.6\n",
      "conda-package-handling==1.7.0\n",
      "conda-verify==3.1.1\n",
      "contextlib2==0.5.5\n",
      "cryptography==2.4.2\n",
      "cycler==0.10.0\n",
      "Cython==0.29.2\n",
      "cytoolz==0.9.0.1\n",
      "dask==1.0.0\n",
      "datashape==0.5.4\n",
      "decorator==4.3.0\n",
      "defusedxml==0.5.0\n",
      "distributed==1.25.1\n",
      "docutils==0.14\n",
      "entrypoints==0.2.3\n",
      "et-xmlfile==1.0.1\n",
      "fastcache==1.0.2\n",
      "filelock==3.0.10\n",
      "Flask==1.0.2\n",
      "Flask-Cors==3.0.7\n",
      "future==0.17.1\n",
      "gast==0.3.3\n",
      "gevent==1.3.7\n",
      "glob2==0.6\n",
      "google-pasta==0.2.0\n",
      "graphviz==0.10.1\n",
      "greenlet==0.4.15\n",
      "h5py==2.10.0\n",
      "heapdict==1.0.0\n",
      "html5lib==1.0.1\n",
      "idna==2.8\n",
      "imageio==2.4.1\n",
      "imagesize==1.1.0\n",
      "imbalanced-learn==0.4.3\n",
      "imblearn==0.0\n",
      "importlib-metadata==0.6\n",
      "ipykernel==5.1.0\n",
      "ipython==7.2.0\n",
      "ipython-genutils==0.2.0\n",
      "ipywidgets==7.4.2\n",
      "isort==4.3.4\n",
      "itsdangerous==1.1.0\n",
      "jdcal==1.4\n",
      "jedi==0.13.2\n",
      "Jinja2==2.10\n",
      "jsonschema==2.6.0\n",
      "jupyter==1.0.0\n",
      "jupyter-client==5.2.4\n",
      "jupyter-console==6.0.0\n",
      "jupyter-core==4.4.0\n",
      "jupyterlab==0.35.3\n",
      "jupyterlab-server==0.2.0\n",
      "keyring==17.0.0\n",
      "kiwisolver==1.0.1\n",
      "lazy-object-proxy==1.3.1\n",
      "libarchive-c==2.8\n",
      "llvmlite==0.26.0\n",
      "locket==0.2.0\n",
      "lxml==4.2.5\n",
      "Markdown==3.3\n",
      "MarkupSafe==1.1.0\n",
      "matplotlib==3.0.2\n",
      "mccabe==0.6.1\n",
      "menuinst==1.4.14\n",
      "mistune==0.8.4\n",
      "mkl-fft==1.0.6\n",
      "mkl-random==1.0.2\n",
      "more-itertools==4.3.0\n",
      "mpmath==1.1.0\n",
      "msgpack==0.5.6\n",
      "multipledispatch==0.6.0\n",
      "mysql-connector-python==8.0.20\n",
      "navigator-updater==0.2.1\n",
      "nbconvert==5.4.0\n",
      "nbformat==4.4.0\n",
      "networkx==2.2\n",
      "nltk==3.4\n",
      "nose==1.3.7\n",
      "notebook==5.7.4\n",
      "numba==0.41.0\n",
      "numexpr==2.6.8\n",
      "numpy==1.18.5\n",
      "numpydoc==0.8.0\n",
      "odo==0.5.1\n",
      "olefile==0.46\n",
      "openpyxl==2.5.12\n",
      "packaging==18.0\n",
      "pandas==0.23.4\n",
      "pandocfilters==1.4.2\n",
      "parso==0.3.1\n",
      "partd==0.3.9\n",
      "path.py==11.5.0\n",
      "pathlib2==2.3.3\n",
      "patsy==0.5.1\n",
      "pep8==1.7.1\n",
      "pickleshare==0.7.5\n",
      "Pillow==5.3.0\n",
      "pkginfo==1.4.2\n",
      "pluggy==0.8.0\n",
      "ply==3.11\n",
      "prometheus-client==0.5.0\n",
      "prompt-toolkit==2.0.7\n",
      "protobuf==3.13.0\n",
      "psutil==5.4.8\n",
      "py==1.7.0\n",
      "pycodestyle==2.4.0\n",
      "pycosat==0.6.3\n",
      "pycparser==2.19\n",
      "pycrypto==2.6.1\n",
      "pycurl==7.43.0.2\n",
      "pyflakes==2.0.0\n",
      "Pygments==2.3.1\n",
      "pylint==2.2.2\n",
      "pyodbc==4.0.25\n",
      "pyOpenSSL==18.0.0\n",
      "pyparsing==2.3.0\n",
      "PySocks==1.6.8\n",
      "pytest==4.0.2\n",
      "pytest-arraydiff==0.3\n",
      "pytest-astropy==0.5.0\n",
      "pytest-doctestplus==0.2.0\n",
      "pytest-openfiles==0.3.1\n",
      "pytest-remotedata==0.3.1\n",
      "python-dateutil==2.7.5\n",
      "pytz==2018.7\n",
      "PyWavelets==1.0.1\n",
      "pywin32==223\n",
      "pywinpty==0.5.5\n",
      "PyYAML==5.3.1\n",
      "pyzmq==17.1.2\n",
      "QtAwesome==0.5.3\n",
      "qtconsole==4.4.3\n",
      "QtPy==1.5.2\n",
      "requests==2.21.0\n",
      "rope==0.11.0\n",
      "ruamel-yaml==0.15.46\n",
      "scikit-image==0.14.1\n",
      "scikit-learn==0.20.1\n",
      "scipy==1.1.0\n",
      "seaborn==0.9.0\n",
      "Send2Trash==1.5.0\n",
      "simplegeneric==0.8.1\n",
      "singledispatch==3.4.0.3\n",
      "six==1.12.0\n",
      "snowballstemmer==1.2.1\n",
      "sortedcollections==1.0.1\n",
      "sortedcontainers==2.1.0\n",
      "Sphinx==1.8.2\n",
      "sphinxcontrib-websupport==1.1.0\n",
      "spyder==3.3.2\n",
      "spyder-kernels==0.3.0\n",
      "SQLAlchemy==1.2.15\n",
      "statsmodels==0.9.0\n",
      "sympy==1.3\n",
      "tables==3.4.4\n",
      "tblib==1.3.2\n",
      "tensorboard-plugin-wit==1.7.0\n",
      "tensorflow-docs===0.0.0cb886cfdd16d66ff7f8d1430676ff395b02910e6-\n",
      "tensorflow-estimator==2.3.0\n",
      "terminado==0.8.1\n",
      "testpath==0.4.2\n",
      "toolz==0.9.0\n",
      "tornado==5.1.1\n",
      "tqdm==4.28.1\n",
      "traitlets==4.3.2\n",
      "unicodecsv==0.14.1\n",
      "urllib3==1.24.1\n",
      "wcwidth==0.1.7\n",
      "webencodings==0.5.1\n",
      "Werkzeug==0.14.1\n",
      "widgetsnbextension==3.4.2\n",
      "win-inet-pton==1.0.1\n",
      "win-unicode-console==0.5\n",
      "wincertstore==0.2\n",
      "wordcloud==1.8.0\n",
      "wrapt==1.10.11\n",
      "xgboost==0.82\n",
      "xlrd==1.2.0\n",
      "XlsxWriter==1.1.2\n",
      "xlwings==0.15.1\n",
      "xlwt==1.3.0\n",
      "xmltodict==0.12.0\n",
      "zict==0.1.3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "f = open(\"requirements.txt\", \"r\")\n",
    "print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Jyothi\\\\Software training\\\\Python\\\\TOPIC modeling'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"C:/Jyothi/Software training/Python/TOPIC modeling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[WinError 5] Access is denied: 'environment'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-91-84d239d58dcc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"environment\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m: [WinError 5] Access is denied: 'environment'"
     ]
    }
   ],
   "source": [
    "os.remove(\"environment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('requirement.txt','w')\n",
    "\n",
    "file.write(f.read())\n",
    "\n",
    "file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
